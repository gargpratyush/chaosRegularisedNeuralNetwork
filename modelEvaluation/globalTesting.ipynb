{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratyush/miniconda3/envs/pytorch_3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import *\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from resnet import *\n",
    "\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torchattacks\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "import os\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='PyTorch Cifar10 Training')\n",
    "# parser.add_argument('--ngpu', type=int, default=1, help='0 = CPU.')\n",
    "# parser.add_argument('--gpu_id', type=int, default=0,\n",
    "#                     help='device range [0,ngpu-1]')\n",
    "\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "ngpu = 1\n",
    "gpu_id = 1\n",
    "if ngpu == 1:\n",
    "    # make only devices indexed by #gpu_id visible\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Load Dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170500096it [00:09, 17881220.16it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "==> Building model..\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 0.01\n",
    "\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "net2 = ResNet18()\n",
    "net2 = net2.to(device)\n",
    "if device == 'cuda':\n",
    "    net2 = torch.nn.DataParallel(net2)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net2.parameters(), lr=lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Load Models\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_basic = \"/home/pratyush/pratyushg/models/basicResnetModels/resnet18_cifar10_basicTraining.pth\"\n",
    "file_path_fgsm = \"/home/pratyush/pratyushg/models/fgsmModels/resnet18_cifar10_fgsmTraining.pth\"\n",
    "file_path_pgd = \"/home/pratyush/pratyushg/models/pgdModels/resnet18_githubPGD20_epoch200_better.pth\"\n",
    "file_path_chaosLoss = \"/home/pratyush/pratyushg/models/chaosLossMinModels/resnet18_cifar10_chaos_regularized_pgd_torchattacks_lambdaChaos0.8_epoch200_lr0.01_iters20.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading basic ResNet model..\n",
      "==> Loading FGSM-trained model..\n",
      "==> Loading PGD-trained model..\n",
      "==> Loading ChaosLoss Minimization model..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ResNet18 model\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print('==> Loading basic ResNet model..')\n",
    "net_basic = ResNet18()\n",
    "net_basic = net_basic.to(device)\n",
    "if device == 'cuda':\n",
    "    net_basic = torch.nn.DataParallel(net_basic)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "state_dict = torch.load(file_path_basic)\n",
    "\n",
    "# Create a new state dictionary without the 'module.' prefix\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith('module.'):\n",
    "        name = k[7:]  # remove 'module.' prefix\n",
    "    else:\n",
    "        name = k\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "net_basic.load_state_dict(state_dict)\n",
    "\n",
    "#Load ResNet18 model with FGSM adversarial training\n",
    "print('==> Loading FGSM-trained model..')\n",
    "net_fgsm = ResNet18()\n",
    "net_fgsm = net_fgsm.to(device)\n",
    "if device == 'cuda':\n",
    "    net_fgsm = torch.nn.DataParallel(net_fgsm)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "state_dict = torch.load(file_path_fgsm)\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith('module.'):\n",
    "        name = k[7:]  # remove 'module.' prefix\n",
    "    else:\n",
    "        name = k\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "net_fgsm.load_state_dict(state_dict)\n",
    "\n",
    "#Load ResNet18 model with PGD adversarial training\n",
    "print('==> Loading PGD-trained model..')\n",
    "net_pgd = ResNet18()\n",
    "net_pgd = net_pgd.to(device)\n",
    "if device == 'cuda':\n",
    "    net_pgd = torch.nn.DataParallel(net_pgd)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "state_dict = torch.load(file_path_pgd)\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith('module.'):\n",
    "        name = k[7:]  # remove 'module.' prefix\n",
    "    else:\n",
    "        name = k\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "net_pgd.load_state_dict(state_dict)\n",
    "\n",
    "#Load ResNet18 model with Chaos Loss minimization training\n",
    "print('==> Loading ChaosLoss Minimization model..')\n",
    "net_chaosLoss = ResNet18()\n",
    "net_chaosLoss = net_chaosLoss.to(device)\n",
    "if device == 'cuda':\n",
    "    net_chaosLoss = torch.nn.DataParallel(net_chaosLoss)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "state_dict = torch.load(file_path_chaosLoss)\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith('resnet18.'):\n",
    "        name = k.replace('resnet18', 'module')\n",
    "    else:\n",
    "        name = k\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "net_chaosLoss.load_state_dict(new_state_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Normal Dataset Evaluation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, net):\n",
    "\n",
    "    '''\n",
    "    This function evaluate net on test dataset\n",
    "    '''\n",
    "\n",
    "    global acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    acc = 100 * correct / total\n",
    "    return test_loss/len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net_chaos on default dataset accuracy: 87 %\n"
     ]
    }
   ],
   "source": [
    "test_loss = test(1, net_chaosLoss)\n",
    "print('net_chaos on default dataset accuracy: %d %%' % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# FGSM Attack Evaluation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading basic ResNet model..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy model\n",
    "\n",
    "print('==> Loading basic ResNet model..')\n",
    "net_basic_copy = ResNet18()\n",
    "net_basic_copy = net_basic_copy.to(device)\n",
    "if device == 'cuda':\n",
    "    net_basic_copy = torch.nn.DataParallel(net_basic_copy)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "state_dict = torch.load(file_path_basic)\n",
    "\n",
    "# Create a new state dictionary without the 'module.' prefix\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith('module.'):\n",
    "        name = k[7:]  # remove 'module.' prefix\n",
    "    else:\n",
    "        name = k\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "net_basic_copy.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FGSM(net, x, y, eps):\n",
    "        '''\n",
    "        inputs:\n",
    "            net: the network through which we pass the inputs\n",
    "            x: the original example which we aim to perturb to make an adversarial example\n",
    "            y: the true label of x\n",
    "            eps: perturbation budget\n",
    "\n",
    "        outputs:\n",
    "            x_adv : the adversarial example constructed from x\n",
    "            h_adv: output of the last softmax layer when applying net on x_adv \n",
    "            y_adv: predicted label for x_adv\n",
    "            pert: perturbation applied to x (x_adv - x)\n",
    "        '''\n",
    "\n",
    "        x_ = Variable(x.data, requires_grad=True)\n",
    "        h_ = net(x_)\n",
    "        criterion= torch.nn.CrossEntropyLoss()\n",
    "        cost = criterion(h_, y)\n",
    "        net.zero_grad()\n",
    "        cost.backward()\n",
    "\n",
    "        #perturbation\n",
    "        pert= eps*x_.grad.detach().sign()\n",
    "        \n",
    "        x_adv = x_ + pert\n",
    "\n",
    "        h_adv = net(x_adv)\n",
    "        _,y_adv=torch.max(h_adv.data,1)\n",
    "        return x_adv, h_adv, y_adv, pert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fgsm(net, net_adv, eps):\n",
    "    accuracy=0\n",
    "    net.train()\n",
    "    net_adv.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        x_adv, h_adv, y_adv, pert = FGSM (net, inputs, targets, eps)\n",
    "            \n",
    "        outputs = net_adv(x_adv)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of net_chaosLoss on FGSM-attacked dataset: 86.82\n"
     ]
    }
   ],
   "source": [
    "accuracy=test_fgsm(net_basic_copy, net_chaosLoss, 8/255)\n",
    "print(\"accuracy of net_chaosLoss on FGSM-attacked dataset:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on FGSM examples: 66.04%\n"
     ]
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pretrained ResNet18 model\n",
    "model = net_chaosLoss\n",
    "model.eval()\n",
    "\n",
    "# Define the FGSM attack\n",
    "epsilon = 8/255  # Perturbation\n",
    "\n",
    "fgsm = torchattacks.FGSM(model, eps=epsilon)\n",
    "\n",
    "# Function to test the model on adversarial examples\n",
    "def test_model_on_adversarial(loader, model, attack):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for data in loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Generate adversarial examples\n",
    "        adv_images = attack(images, labels)\n",
    "\n",
    "        # Forward pass the adversarial examples through the model\n",
    "        outputs = model(adv_images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on FGSM examples: {100 * correct / total:.2f}%')\n",
    "\n",
    "# Test the model on the adversarially-attacked CIFAR-10 test set\n",
    "test_model_on_adversarial(testloader, model, fgsm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# PGD Attack Evaluation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on PGD examples: 46.07%\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained ResNet18 model\n",
    "model = net_pgd\n",
    "model.eval()\n",
    "\n",
    "# Define the PGD attack\n",
    "epsilon = 8/255  # Perturbation\n",
    "alpha = 2/255    # Step size\n",
    "steps = 20        # Number of iterations\n",
    "\n",
    "pgd = torchattacks.PGD(model, eps=epsilon, alpha=alpha, steps=steps)\n",
    "\n",
    "# Function to test the model on adversarial examples\n",
    "def test_model_on_adversarial(loader, model, attack):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for data in loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Generate adversarial examples\n",
    "        adv_images = attack(images, labels)\n",
    "\n",
    "        # Forward pass the adversarial examples through the model\n",
    "        outputs = model(adv_images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on PGD examples: {100 * correct / total:.2f}%')\n",
    "\n",
    "# Test the model on the adversarially-attacked CIFAR-10 test set\n",
    "test_model_on_adversarial(testloader, model, pgd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Susceptibility Comparison\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susceptibility Ratio of Basic Model: 1.490615725517273\n",
      "Susceptibility Ratio of Adversarially Trained (FGSM) Model: 1.060843825340271\n",
      "Susceptibility Ratio of Adversarially Trained (PGD) Model: 1.0887888669967651\n",
      "Susceptibility Ratio of Chaos-loss trained Model: 1.0358573198318481\n"
     ]
    }
   ],
   "source": [
    "# Define a function to calculate the susceptibility ratio\n",
    "def susceptibility_ratio(net, x, delta_x_adv):\n",
    "    x = x.to(device)\n",
    "    delta_x_adv = delta_x_adv.to(device)\n",
    "    \n",
    "    # Ensure the model is in evaluation mode\n",
    "    net.eval()\n",
    "    \n",
    "    # Calculate h(θ; x_i)\n",
    "    output_x = net(x)\n",
    "    \n",
    "    # Calculate h(θ; x_i + δx_adv)\n",
    "    output_x_adv = net(x + delta_x_adv)\n",
    "    \n",
    "    # Calculate the susceptibility ratio\n",
    "    num = torch.norm(output_x - output_x_adv, p=2)\n",
    "    denom = torch.norm(delta_x_adv, p=2)\n",
    "    \n",
    "    susceptibility = torch.exp(num / denom)\n",
    "    return susceptibility.item()\n",
    "\n",
    "# Get a batch of training data\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Define a small adversarial perturbation\n",
    "epsilon = 0.01\n",
    "delta_x_adv = epsilon * torch.sign(torch.randn_like(images)) #FGSM Attack\n",
    "\n",
    "# Calculate the susceptibility ratio for the batch\n",
    "susceptibility_basic_model = susceptibility_ratio(net_basic, images, delta_x_adv)\n",
    "susceptibility_fgsm_model = susceptibility_ratio(net_fgsm, images, delta_x_adv)\n",
    "susceptibility_pgd_model = susceptibility_ratio(net_pgd, images, delta_x_adv)\n",
    "susceptibility_chaosLoss_model = susceptibility_ratio(net_chaosLoss, images, delta_x_adv)\n",
    "print(f'Susceptibility Ratio of Basic Model: {susceptibility_basic_model}')\n",
    "print(f'Susceptibility Ratio of Adversarially Trained (FGSM) Model: {susceptibility_fgsm_model}')\n",
    "print(f'Susceptibility Ratio of Adversarially Trained (PGD) Model: {susceptibility_pgd_model}')\n",
    "print(f'Susceptibility Ratio of Chaos-loss trained Model: {susceptibility_chaosLoss_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susceptibility Ratio Comparison when the perbutation is generated by iterative attacks:\n",
      "Layer                     Basic Model     PGD Model       ChaosMin Model (λ=0.8) \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'conv1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 97\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:<25}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{:<15}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{:<15}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{:<15}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBasic Model\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPGD Model\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChaosMin Model (λ=0.8)\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers_to_capture:\n\u001b[0;32m---> 97\u001b[0m     sus_basic \u001b[38;5;241m=\u001b[39m \u001b[43msusceptibilities_basic\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# sus_fgsm = susceptibilities_fgsm[layer]\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     sus_pgd \u001b[38;5;241m=\u001b[39m susceptibilities_pgd[layer]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'conv1'"
     ]
    }
   ],
   "source": [
    "layers_to_capture = [\n",
    "    'conv1', 'bn1',\n",
    "    'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.conv2', 'layer1.0.bn2',\n",
    "    'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.conv2', 'layer1.1.bn2',\n",
    "    'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.shortcut.0', 'layer2.0.shortcut.1',\n",
    "    'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.conv2', 'layer2.1.bn2',\n",
    "    'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.shortcut.0', 'layer3.0.shortcut.1',\n",
    "    'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.conv2', 'layer3.1.bn2',\n",
    "    'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.shortcut.0', 'layer4.0.shortcut.1',\n",
    "    'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.conv2', 'layer4.1.bn2',\n",
    "    'linear'\n",
    "]\n",
    "\n",
    "# Define a function to register hooks and capture intermediate outputs\n",
    "def register_hooks(model):\n",
    "    activations = {}\n",
    "    \n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activations[name] = output\n",
    "        return hook\n",
    "    \n",
    "    for name, layer in model.named_modules():\n",
    "        layer.register_forward_hook(get_activation(name))\n",
    "    \n",
    "    return activations\n",
    "\n",
    "# Register hooks for all layers\n",
    "activations_basic = register_hooks(net_basic)\n",
    "activations_fgsm = register_hooks(net_fgsm)\n",
    "activations_pgd = register_hooks(net_pgd)\n",
    "activations_chaosLoss = register_hooks(net_chaosLoss)\n",
    "\n",
    "# Define a function to calculate the susceptibility ratio\n",
    "def susceptibility_ratio(net, x, labels, activations):\n",
    "    \n",
    "    x = x.to(device)\n",
    "    pgd = torchattacks.PGD(net, eps=0.03, alpha=0.01, steps=10)\n",
    "    delta_x_adv = pgd(x, labels) - x\n",
    "    delta_x_adv = delta_x_adv.to(device)\n",
    "    # print(torch.norm(delta_x_adv, p=2))\n",
    "    \n",
    "    # Ensure the model is in evaluation mode\n",
    "    net.eval()\n",
    "    \n",
    "    # Clear previous activations\n",
    "    activations.clear()\n",
    "    \n",
    "    # Forward pass for original input\n",
    "    _ = net(x)\n",
    "    output_x_layers = activations.copy()\n",
    "    \n",
    "    # Clear previous activations\n",
    "    activations.clear()\n",
    "    \n",
    "    # Forward pass for perturbed input\n",
    "    _ = net(x + delta_x_adv)\n",
    "    output_x_adv_layers = activations.copy()\n",
    "    \n",
    "    susceptibilities = {}\n",
    "    for layer in output_x_layers.keys():\n",
    "        output_x = output_x_layers[layer]\n",
    "        output_x_adv = output_x_adv_layers[layer]\n",
    "        \n",
    "        # Calculate the susceptibility ratio for each layer\n",
    "        num = torch.norm(output_x - output_x_adv, p=2)\n",
    "        denom = torch.norm(delta_x_adv, p=2)\n",
    "        susceptibility = num / denom\n",
    "        # print(num.item(), denom.item(), susceptibility.item())\n",
    "        susceptibilities[layer] = susceptibility.item()\n",
    "    \n",
    "    return susceptibilities\n",
    "\n",
    "# Get a batch of training data\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Define a small adversarial perturbation\n",
    "# epsilon = 0.01\n",
    "# delta_x_adv = epsilon * torch.sign(torch.randn_like(images))\n",
    "\n",
    "\n",
    "    \n",
    "# Calculate the susceptibility ratio for the batch\n",
    "susceptibilities_basic = susceptibility_ratio(net_basic, images, labels, activations_basic)\n",
    "# susceptibilities_fgsm = susceptibility_ratio(net_fgsm, images, labels, activations_fgsm)\n",
    "susceptibilities_pgd = susceptibility_ratio(net_pgd, images, labels, activations_pgd)\n",
    "susceptibilities_chaosLoss = susceptibility_ratio(net_chaosLoss, images, labels, activations_chaosLoss)\n",
    "if 'conv1' in susceptibilities_basic:\n",
    "    sus_basic = susceptibilities_basic['conv1']\n",
    "    print(\"hello\")\n",
    "else:\n",
    "    sus_basic = 0.0\n",
    "    print(\"hello2\")\n",
    "# Print the susceptibility ratios in a table format\n",
    "print(\"Susceptibility Ratio Comparison when the perbutation is generated by iterative attacks:\")\n",
    "print(\"{:<25} {:<15} {:<15} {:<15} \".format('Layer', 'Basic Model', 'PGD Model', 'ChaosMin Model (λ=0.8)'))\n",
    "for layer in layers_to_capture:\n",
    "    sus_basic = susceptibilities_basic[layer]\n",
    "    # sus_fgsm = susceptibilities_fgsm[layer]\n",
    "    sus_pgd = susceptibilities_pgd[layer]\n",
    "    sus_chaos = susceptibilities_chaosLoss[layer]\n",
    "    print(\"{:<25} {:<15.6f} {:<15.6f} {:<15.6f}\".format(layer, sus_basic, sus_pgd, sus_chaos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Histogram for Weight Distribution\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your models\n",
    "model1 = net_pgd  # Assuming this is defined somewhere in your code\n",
    "model2 = net_chaosLoss\n",
    "\n",
    "# Function to get convolutional layers\n",
    "def get_conv_layers(model):\n",
    "    conv_layers = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            conv_layers.append((name, module))\n",
    "    return conv_layers\n",
    "\n",
    "# Function to plot histograms of weights for convolutional layers of two models\n",
    "def plot_conv_weight_histograms(model1, model2):\n",
    "    conv_layers1 = get_conv_layers(model1)\n",
    "    conv_layers2 = get_conv_layers(model2)\n",
    "    \n",
    "    num_layers = len(conv_layers1)\n",
    "    fig, axes = plt.subplots(num_layers, 2, figsize=(12, num_layers * 1.5))\n",
    "\n",
    "    for i, ((name1, layer1), (name2, layer2)) in enumerate(zip(conv_layers1, conv_layers2)):\n",
    "        weight1 = layer1.weight.data.cpu().numpy().flatten()\n",
    "        weight2 = layer2.weight.data.cpu().numpy().flatten()\n",
    "\n",
    "        axes[i, 0].hist(weight1, bins=50)\n",
    "        axes[i, 0].set_title(f'Model 1: {name1}')\n",
    "        axes[i, 0].set_xlabel('Weight value')\n",
    "        axes[i, 0].set_ylabel('Frequency')\n",
    "\n",
    "        axes[i, 1].hist(weight2, bins=50)\n",
    "        axes[i, 1].set_title(f'Model 2: {name2}')\n",
    "        axes[i, 1].set_xlabel('Weight value')\n",
    "        axes[i, 1].set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot histograms of the weights for convolutional layers of both models\n",
    "plot_conv_weight_histograms(model1, model2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
